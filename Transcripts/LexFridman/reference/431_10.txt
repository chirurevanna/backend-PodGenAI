"What to you is the probability that superintelligent AI will destroy all human civilization? What’s the timeframe? Let’s say a hundred years, in the next hundred years. So the problem of controlling AGI or superintelligence, in my opinion, is like a problem of creating a perpetual safety machine. By analogy with perpetual motion machines, it’s impossible. Yeah, we may succeed and do a good job with GPT-5, six, seven, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actors. The difference between cybersecurity, narrow AI safety, and safety for general AI for superintelligence is that we don’t get a second chance. With cybersecurity, somebody hacks your account, what’s the big deal? You get a new password, new credit card, you move on. Here, if we’re talking about existential risks, you only get one chance. So you are really asking me what are the chances that we’ll create the most complex software ever on the first try with zero bugs and it’ll continue to have zero bugs for a hundred years or more. So there is an incremental improvement of systems leading up to AGI. To you, it doesn’t matter if we can keep those safe. There’s going to be one level of system at which you cannot possibly control it. I don’t think we so far have made any system safe at the level of capability they display. They already have made mistakes. We had accidents. They’ve been jailbroken. I don’t think there is a single large language model today which no one was successful at making do something developers didn’t intend it to do. There’s a difference between getting it to do something unintended, getting it to do something that’s painful, costly, destructive, and something that’s destructive to the level of hurting billions of people or hundreds of millions of people, billions of people, or the entirety of human civilization. That’s a big leap. Exactly, but the systems we have today have the capability of causing X amount of damage. So when we fail, that’s all we get. If we develop systems capable of impacting all of humanity, all of the universe, the damage is proportionate. What to you are the possible ways that such mass murder of humans can happen? It’s always a wonderful question. So one of the chapters in my new book is about unpredictability. I argue that we cannot predict what a smarter system will do. So you’re really not asking me how superintelligence will kill everyone. You’re asking me how I would do it. I think it’s not that interesting. I can tell you about the standard nanotech, synthetic, bio, nuclear. Superintelligence will come up with something completely new, completely super. We may not even recognize that as a possible path to achieve that goal. So there is an unlimited level of creativity in terms of how humans could be killed, but we could still investigate possible ways of doing it. Not how to do it, but at the end, what is the methodology that does it? Shutting off the power and then humans start killing each other maybe, because the resources are really constrained. Then there’s the actual use of weapons like nuclear weapons or developing artificial pathogens, viruses, that kind of stuff. We could still think through that and defend against it. There’s a ceiling to the creativity of mass murder of humans here. The options are limited. They’re limited by how imaginative we are. If you are that much smarter, that much more creative, you’re capable of thinking across multiple domains, do novel research in physics and biology, you may not be limited by those tools. If squirrels were planning to kill humans, they would have a set of possible ways of doing it, but they would never consider things we can come up. So are you thinking about mass murder and destruction of human civilization or are you thinking of with squirrels, you put them in a zoo and they don’t really know they’re in a zoo? If we just look at the entire set of undesirable trajectories, the majority of them are not going to be death. Most of them are going to be just things like Brave New World where the squirrels are fed dopamine and they’re all doing some fun activity and the fire, the soul of humanity is lost because of the drug that’s fed to it, or literally in a zoo. We’re in a zoo, we’re doing our thing, we’re playing a game of Sims, and the actual players playing that game are AI systems. Those are all undesirable because the free will. The fire of human consciousness is dimmed through that process, but it’s not killing humans. So are you thinking about that or is the biggest concern literally the extinction of humans? I think about a lot of things. So that is X-risk, existential risk, everyone’s dead. There is S-risk, suffering risks, where everyone wishes they were dead. We have also the idea for I-risk, ikigai risks, where we lost our meaning. The systems can be more creative. They can do all the jobs. It’s not obvious what you have to contribute to a world where superintelligence exists. Of course, you can have all the variants you mentioned where we are safe, we’re kept alive, but we are not in control. We’re not deciding anything. We’re like animals in a zoo. There is, again, possibilities we can come up with as very smart humans and then possibilities something a thousand times smarter can come up with for reasons we cannot comprehend. I would love to dig into each of those X-risk, S-risk, and I-risk. So can you linger on I-risk? What is that? So Japanese concept of ikigai, you find something which allows you to make money. You are good at it and the society says we need it. So you have this awesome job. You are a podcaster, gives you a lot of meaning. You have a good life. I assume you’re happy. That’s what we want more people to find, to have. For many intellectuals, it is their occupation, which gives them a lot of meaning. I’m a researcher, philosopher, scholar. That means something to me. In a world where an artist is not feeling appreciated, because his art is just not competitive with what is produced by machines or a writer or scientist will lose a lot of that. At the lower level, we’re talking about complete technological unemployment. We’re not losing 10% of jobs. We’re losing all jobs. What do people do with all that free time? What happens then? Everything society is built on is completely modified in one generation. It’s not a slow process where we get to figure out how to live that new lifestyle, but it’s pretty quick. In that world, can’t humans do what humans currently do with chess, play each other, have tournaments, even though AI systems are far superior this time in chess? So we just create artificial games, or for us they’re real. Like the Olympics and we do all kinds of different competitions and have fun. Maximize the fun and let the AI focus on the productivity. It’s an option. I have a paper where I try to solve the value alignment problem for multiple agents and the solution to avoid compromise is to give everyone a personal virtual universe. You can do whatever you want in that world. You could be king. You could be slave. You decide what happens. So it’s basically a glorified video game where you get to enjoy yourself and someone else takes care of your needs and the substrate alignment is the only thing we need to solve. We don’t have to get 8 billion humans to agree on anything. Okay. So why is that not a likely outcome? Why can’t the AI systems create video games for us to lose ourselves in each with an individual video game universe? Some people say that’s what happened. We’re in a simulation. We’re playing that video game and now we’re creating what… Maybe we’re creating artificial threats for ourselves to be scared about, because fear is really exciting. It allows us to play the video game more vigorously. Some people choose to play on a more difficult level with more constraints. Some say, okay, I’m just going to enjoy the game high privilege level. Absolutely. Okay, what was that paper on multi-agent value alignment? Personal universes. So that’s one of the possible outcomes, but what in general is the idea of the paper? So it’s looking at multiple agents. They’re human AI, like a hybrid system, whether it’s humans and AIs or is it looking at humans or just intelligent agents? In order to solve the value alignment problem, I’m trying to formalize it a little better. Usually, we’re talking about getting AIs to do what we want, which is not well-defined. Are we talking about the creator of a system, the owner of that AI, humanity as a whole, but we don’t agree on much. There is no universally accepted ethics, morals across"