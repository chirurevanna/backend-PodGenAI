"This is so great. Lighting change? Wow. Yeah, we can put the light anywhere. And it doesn’t feel awkward to be really close to you. No, it does. I actually moved you back a few feet before you got into the headset. You were right here. I don’t know if people can see this, but this is incredible. The realism here is just incredible. Where am I? Where are you, Mark? Where are we? You’re in Austin, right? No. I mean this place. We’re shrouded by darkness with ultra-realistic face, and it just feels like we’re in the same room. This is really the most incredible thing I’ve ever seen. And sorry to be in your personal space. We have done jujitsu before. Yeah. I was commenting to the team before that I feel like we’ve choked each other from further distances than it feels like we are right now. This is just really incredible. I don’t know how to describe it with words. It really feels like we’re in the same room. Yeah. It feels like the future. This is truly, truly incredible. I just wanted to take it in. I’m still getting used to it. It’s you, it’s really you, but you’re not here with me. You’re there wearing a headset, and I’m wearing a headset. It’s really, really incredible. Can you describe what it takes currently for us to appear so photo realistic to each other? Yeah. So, for background, we both did these scans for this research project that we have at Meta called Kodak Avatars. And the idea is that instead of our avatars being cartoony and instead of actually transmitting a video, what it does is we’ve scanned ourselves and a lot of different expressions, and we’ve built a computer model of each of our faces and bodies and the different expressions that we make and collapsed that into a Kodak that then when you have the headset on your head, it sees your face, it sees your expression, and it can basically send an encoded version of what you’re supposed to look like over the wire. So, in addition to being photorealistic, it’s also actually much more bandwidth efficient than transmitting a full video or especially a 3D immersive video of a whole scene like this. And it captures everything. To me, the subtleties of the human face, even the flaws, that’s all amazing. It makes it so much more immersive. It makes you realize that perfection isn’t the thing that leads to immersion. It’s the little subtle flaws like freckles and variations in color and just… Wrinkles. … all stuff about noses. Asymmetry. Yeah, asymmetry, and just the corners of the eyes, what your eyes do when you smile, all that kind of stuff. Eyes are a huge part of it. It’s just incredible. There’s all these studies that most of communication, even when people are speaking, is not actually the words that they’re saying. It’s the expression and all that. And we try to capture that with the classical expressive avatar system that we have. That’s the more cartoon-designed one. You can put those expressions on those faces as well. But there’s obviously a certain realism that comes with delivering this photo-realistic experience that, I don’t know, I just think it’s really magical. This gets to the core of what the vision around virtual and augmented reality is, of delivering a sense of presence as if you’re there together no matter where you actually are in the world. This experience I think is a good embodiment of that, where we’re in two completely different states halfway across the country, and it looks like you’re just sitting right in front of me. It’s pretty wild. Yeah. I’m almost getting emotional. It feels like a totally fundamentally new experience. For me to have these kinds of conversations with loved ones, it would just change everything. Maybe just to elaborate, I went to Pittsburgh and went through the whole scanning procedure, which has so much incredible technology, software and hardware, going on, but it is a lengthy process. So what’s your vision for the future of this in terms of making this more accessible to people? It starts off with a small number of people doing these very detailed scans. That’s the version that you did and that I did, and before there were a lot of people who we’ve done this a scan for, we probably need to over collect expressions when we’re doing the scanning because we haven’t figured out how much we can reduce that down to a really streamlined process and extrapolate from the scans that have already been done. But the goal, and we have a project that’s working on this already, is just to do a very quick scan with your cell phone where you just take your phone, wave it in front of your face for a couple of minutes, say a few sentences, make a bunch of expressions, but, overall, have the whole process just be two to three minutes and then produce something that’s of the quality of what we have right now. So I think that that’s one of the big challenges that remains, and right now we have the ability to do the scans if you have hours to sit for one. And with today’s technology, you’re using a Meta headset that exists. It’s a product that’s for sale now. You can drive these with that, but the production of these scans in a very efficient way is one of the last pieces that we still need to really nail. And then, obviously, there’s all the experiences around it. Right now we’re sitting in a dark room, which is familiar for your podcast, but I think part of the vision for this over time is not just having this be a video call. That’s fine, it’s cool, it feels like it’s immersive, but you can do a video call on your phone. The thing that you can do in the Metaverse that is different from what you can do on a phone is doing stuff where you’re physically there together and participating in things together. And we could play games like this. We could have meetings like this in the future. Once you get mixed reality and augmented reality, we could have Kodak Avatars like this and go into a meeting and have some people physically there and have some people show up in this photorealistic form superimposed on the physical environment. Stuff like that is going to be super powerful. So we’ve got to still build out all those applications and the use cases around it. But I don’t know, I think it’s going to be a pretty wild next few years around this. I’m actually almost at a loss for words. This is just so incredible. This is truly incredible. I hope that people watching this can get a glimpse of how incredible it is. It really feels like we’re in the same room. I guess there’s an uncanny valley that seems to have been crossed here. It looks like you. There’s still a bunch of tuning that I think we’ll want to do where different people emote to different extents, so I think one of the big questions is, when you smile, how wide is your smile? And how wide do you want your smile to be? And I think getting that to be tuned on a per person basis is going to be one of the things that we’re going to need to figure out. It’s like to, what extent do you want to give people control over that? Some people might prefer a version of themselves that’s more emotive in their avatar than their actual faces. So, for example, I always get a lot of critique and shit for having a relatively stiff expression. I might feel pretty happy, but just make a pretty small smile. So maybe, for me, it’s like I’d want to have my avatar really be able to better express how I’m feeling than how I can do physically. So I think that there’s a question about how you want to tune that, but, overall, yeah, we want to start from the baseline of capturing how people actually emote and express themselves. And I think the initial version of this has been pretty impressive. And like you said, I do think we’re beyond the uncanny valley here where it does feel like you. It doesn’t feel weird or anything like that. That’s going to be the meme that the two most monotone people are in the Metaverse together, but I think that actually makes it more difficult. The amazing thing here is that the subtleties of the expression of the eyes, people say I’m monotone and emotionless, but I’m not. It’s just maybe my expression of emotion is more subtle, usually, with the eyes. And that’s one of the things I’ve noticed is just how expressive the subtle movement of the corners of the eyes are in terms of displaying happiness or boredom or all that stuff. I am curious to see, just because I’ve never done one of these before, I’ve never done a podcast as one of these Kodak Avatars, and I’m curious to see what people think of it. Because one of the issues that we’ve had in some of the VR and mixed reality work is it tends to feel a lot more profound when you’re in it than the 2D videos capturing the experience. So I think that this one, because it’s photo"