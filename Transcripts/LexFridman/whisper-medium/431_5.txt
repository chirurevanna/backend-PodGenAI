{
  "transcription": " What to you is the probability that superintelligent AI will destroy all human civilization? What's the time frame? Let's say a hundred years in the next hundred years. So the problem of controlling AGI or superintelligence, in my opinion, is like a problem of creating a perpetual safety machine by analogy with perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actors. The difference between cybersecurity, narrow AI safety and safety for general AI for super intelligence is that we don't get a second chance. With cybersecurity, somebody hacks your account, what's the big deal? You get a new password, new credit card, you move on. Here, if we're talking about existential risks, you only get one chance. So you're really asking me, what are the chances that we'll create the most system safe. At the level of capability they display, they already have made mistakes. We had accidents. They've been jailbroken. I don't think there is a single large language model today which no one was successful at making do something developers didn't intend to do. But there's a difference between getting it to do something unintended, getting it to do something that's painful, costly, destructive, and something that's destructive to the level of hurting billions of people or hundreds of millions of people, billions of people, or the entirety of human civilization. That's a big leap. Exactly. But the systems we have today have capability of causing X amount of damage. So then they fail. That's all we get. If we develop systems capable of impacting all of humanity, all of universe, the damage is proportionate. What to you are the possible ways that such kind of mass murder of humans can happen? I would do it. And I think it's not that interesting. I can tell you about the standard, you know, nanotech synthetic bio nuclear super intelligence will come up with something completely new, completely super. We may not even recognize that as a possible path to achieve that goal. So there is like a unlimited level of creativity in terms of how humans could be killed. But, you know, we could still investigate possible ways of doing it. Not how to do it, but at the end, what is the methodology that does it? You know, shutting off the power, and then humans start killing each other maybe because the resources are really constrained. And then there's the actual use of weapons like nuclear weapons or developing artificial pathogens, viruses, that kind of stuff. We could still kind of think through that and defend against it, right? There's a ceiling to the creativity of mass murder of humans here. The options are limited. possible ways of doing it, but we would never consider things we can come up. So are you thinking about mass murder and destruction of human civilization, or are you thinking of, with squirrels, you put them in a zoo, and they don't really know they're in a zoo? If we just look at the entire set of undesirable trajectories, majority of them are not going to be death. Most of them are going to be just like things like Brave New World where the squirrels are fed dopamine and they're all doing some kind of fun activity and the fire, the soul of humanity is lost because of the drug that's fed to it or like literally",
  "duration": 23.1845440864563
}