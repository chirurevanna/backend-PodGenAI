{
  "transcription": " You've had some strong statements, technical statements about the future of artificial intelligence recently, throughout your career actually, but recently as well. You've said that auto-aggressive LLMs are not the way we're going to make progress towards superhuman intelligence. These are the large language models like GPT-4, like ALAMA-2 and 3 soon and so on. How do they work and why are they not going to take us all the way? For a number of reasons. The first is that there is a number of characteristics of intelligent behavior. For example, the capacity to understand the world, understand the physical world, the ability to remember and retrieve things, persistent memory, the ability to reason, and the ability to plan. Those are four essential characteristics of intelligent systems or entities, humans, animals. LNMs can do none of those. Or they can only do them in a very primitive way. And they don't really understand the physical world. They don't really have persistent memory. They can't really reason and they certainly can't plan. And so, you know, if you expect the system to become intelligent, just, you know, without having the possibility of doing those things, you're making a mistake. That is not to say that autoregressive LLMs are not useful. They're certainly useful. That they're not interesting, that we can't build a whole ecosystem of applications around them. Of course we can, but as a path towards human level intelligence, they're missing essential components. And then there is another tidbit or fact that I think is very interesting. Those headlines are trained on enormous amounts of text, basically the entirety of all publicly available texts on the internet. That's typically on the order of 10 to the 13 tokens. Each token is typically two bytes. So that's two 10 to the 13 bytes as training data. It would take you or me 170,000 years to just read through this at eight hours a day. So it seems like an enormous amount of knowledge that those systems can accumulate. But then you realize it's really not that much data. If you talk to developmental psychologists, and they tell you a four-year-old has been awake for 16,000 hours in his or her life, and the amount of information that has reached the visual cortex of that child in four years is about 10 to the 15 bytes. And you can compute this by estimating that the optical nerve carry about 20 megabytes per second, roughly. And so 10 to the 15 bytes for a four-year-old versus two times 10 to the 13 bytes for 170,000 years worth of reading. What that tells you is that through sensory input, we see a lot more information than we do through language. And that despite our intuition, most of what we learn and most of our knowledge is through our observation and interaction with the real world, not through language. Everything that we learn in the first few years of life and certainly everything that animals learn has nothing to do with language. Somebody might argue your comparison between sensory data versus language, that language is already very compressed. It already contains a lot more information than the bytes it takes to store them, if you compare it to visual data. So there's a lot of wisdom in language, there's words, and the way we stitch them together, it already contains a lot of information. So is it possible that language alone already has enough wisdom and knowledge in there to be able to, from that language, construct a world model and understanding of the world and understanding of the physical world that you're saying LLMs lack? So it's a big debate among philosophers and also cognitive scientists, like whether intelligence needs to be grounded in reality. I'm clearly in the camp that yes, intelligence cannot appear without some grounding in some reality. It doesn't need to be physical reality. It could be simulated, but the environment is just much richer than what you can express in language. Language is a very approximate representation of our percepts and our mental models. There's a lot of tasks that we accomplish where we manipulate a mental model of the situation at hand, and that has nothing to do with language. Everything that's physical, mechanical, whatever, when we build something, when we accomplish a task, a model task of grabbing something, et cetera, we plan or action sequences, and we do this by essentially imagining the result of the outcome of a sequence of actions that we might imagine. And that requires mental models that don't have much to do with language. And that's, I would argue, most of our knowledge is derived from that interaction with the physical world. So a lot of my colleagues who are more interested in things like computer vision are really on that camp that AI needs to be embodied, essentially. And then other people coming from the NLP side or maybe some other motivation don't necessarily agree with that. And philosophers are split as well. And the complexity of the world is hard to imagine. It's hard to represent all the complexities that we take completely for granted in the real world that we don't even imagine require intelligence. This is the old Moravec paradox from the pioneer of robotics, Hans Moravec, who said, how is it that with computers, it seems to be easy to do high-level complex tasks like playing chess and solving integrals and doing things like that? Whereas the thing we take for granted that we do every day, like learning to drive a car or grabbing an object. We can do with computers. And we have LLMs that can pass the bar exam, so they must be smart. But then they can't learn to drive in 20 hours like any 17-year-old. They can't learn to clear out the dinner table and fill up the dishwasher like any 10 year old can learn in one shot. Why is that? What are we missing? What type of learning or reasoning architecture or whatever are we missing that basically prevent us from having level five sort of in cars and domestic robots. Can a large language model construct a world model that does know how to drive and does know how to fill a dishwasher, but just doesn't know how to deal with visual data at this time? So it can operate in a space of concepts. So yeah, that's what a lot of people are working on. So the answer, the short answer is no. And the more complex answer is you can use all kinds of tricks to get an LLM to basically digest visual representations of representations of images or video or audio for that matter. And a classical way of doing this is you train a vision system in some way. And we have a number of ways to train vision systems. You either supervise, semi-supervise, self-supervise, all kinds of different ways. That will turn any image into a high level representation. Basically Basically a list of tokens that are really similar to the kind of tokens that typical LLM takes as an input. And then you just feed that to the LLM in addition to the text. And you just expect the LLM to kind of, during training, to kind of be able to use those representations to help make decisions. I mean, there's been work along those lines for quite a long time. And now you see those systems, right? I mean, there are LLMs that have some vision extension, but they're basically hacks in the sense that those things are not like trained end-to-end to handle, to really understand the world. They're not trained with video, for example. They don't really understand intuitive physics, at least not at the moment. So you don't think there's something special to you about intuitive physics, about sort of common sense reasoning about the physical space, about physical reality? That to you is a giant leap that LLMs are just not able to do. We're not going to be able to do this with the type of LLMs that we are working with today. And there's a number of reasons for this. But the main reason is the way LLMs are trained is that you take a",
  "duration": 51.67517566680908
}