{
  "transcription": " What to you is the probability that superintelligent AI will destroy all human civilization? What's the time frame? Let's say a hundred years in the next hundred years. So the problem of controlling AGI or superintelligence, in my opinion, is like a problem of creating a perpetual safety machine by analogy with perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actors. The difference between cybersecurity, narrow AI safety and safety for general AI for super intelligence is that we don't get a second chance. With cybersecurity, somebody hacks your account, what's the big deal? You get a new password, new credit card, you move on."
}