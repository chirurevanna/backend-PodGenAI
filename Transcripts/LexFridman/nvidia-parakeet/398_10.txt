{
  "transcription": "this is so great lighting change wow yeah we can put the light anywhere and it doesn't feel awkward to be really close to you no it does i actually moved you i moved you back a few feet before you got involved you were like right here i don't know if people can see this but this is incredible the realism here is just incredible where am i where are you mark where are we you're in austin right no i mean this place where we're surrounded by darkness with ultra realistic face and just feels like we're in the same room this is really the most incredible thing i've ever seen and sorry to be in your personal space i mean we have done jejesa before yeah no i was i was commenting to the team before that even i feel like we've choked each other from further distances than it feels like we are i mean this is just really incredible i don't know how to describe with words it really feels like it feels like we're in the same room yeah feels like the future this is truly truly incredible i just want to take it in i'm still getting used to it's like it's you it's really you but you're not here with me right you're there with wearing a headset and i'm wearing a headset it's really really incredible so what can you describe what it takes currently for us to appear so photorealistic to each other yeah so i mean for background we both did these scans for this research project that that we have at meta called kodak avatars and the idea is that instead of actually instead of our avatars being cartoony and instead of actually transmitting a video what it does is we've sort of scanned ourselves and a lot of different expressions and we've built a computer model of sort of each of our faces and bodies and the different expressions that we make and collapse that into a codec that then when you have the headset on your head it can it sees your face it sees your expression and it can basically send an encoded version of what you're supposed to look like over the wire so so in addition to being photorealistic it's also actually much more bandwidth efficient than transmitting a full video or especially a three d immersive video of a whole scene like this and it captures everything like the flaws like to me the subtleties of the human face like even the flaws that's like that's all amazing it makes you it makes it so much more immersive it makes you realize that like performance perfection isn't the thing that leads to immersion it's like the little subtle flaws like freckles and like variations in color and just yeah wrinkles all stuff yeah asymmetry and just different like the corners of the eyes like what your eyes do when you smile all that kind of stuff yeah eyes there are a huge part of it yeah i mean there's all the studies that most of communication even when people are speaking is not actually the words that they're saying right it's kind of the expression and and all that so and we try to capture that with the kind of classical expressive avatar system that we have that's the kind of more cartoon designed one you can you can kind of put those kind of expressions on those faces as well but there's obviously a certain realism that comes with delivering kind of this photorealistic experience that i don't know i just think it's really magical i mean this gets to kind of the core of what the vision around virtual and augmented reality is of like delivering a sense of presence as if you're there together no matter where you actually are in the world and i mean this this experience i think is a good embodiment of that where it's like i mean we're in two completely different states halfway across the country and it just like you know looks like you're just sitting right in front of me it's it's pretty wild yeah yeah i can't it's i'm almost getting emotional it's like it feels like a totally it's fundamentally new experience like for me to have this kind of conversations with loved ones it would just change everything maybe just to elaborate so the i went to pittsburgh and went through this whole scanning procedure which has so much incredible technology so software and hardware going on but it is a lengthy process so what's your vision for the future of this in terms of making this more accessible to people you know it starts off with a small number of people doing these very detailed scans right which is this that's the version that you did and that i did and you know before there are a lot of people who we've done this kind of a scan for we probably need to kind of overcollect expressions when we're doing the scanning because we haven't figured out how much we can reduce that down to a really streamlined process and extrapolate from the scans that have already been done but you know the goal and we have a project that's working on this already is just to do a very quick scan with your cell phone where you just take your phone kind of wave it in front of your face for a couple minutes say a few sentences make a bunch of expressions but overall have the whole process just be two to three minutes and then produce something that's of the quality of what we have right now so i think that that's one of the big challenges that remains and right now we have the ability to do the scans if you have hours to sit for one and with today's technology i mean you're using a meta headset that exists it's a product that's kind of for sale now you can drive these with that but the production of of these scans in a very efficient way is one of the last pieces that we still need to really nail and then obviously there's all the experiences around it i mean right now we're kind of sitting in a dark room which you know is you know familiar for your podcast but i think part of the vision for this over time is is you know not just having this be like a video call i mean that's fine it's it's cool or it feels like it's immersive but you can do a video call on your phone the thing that you can do in the metaverse that is different from what you can do on a phone is like doing stuff where you're physically there together and participating in things together and we could play games like this we could have meetings like this in the future once you mix once you get mixed reality and augmented reality we could have codec avatars like this and go into a meeting and have some people physically there and have some people show up in this photorealistic form uh superimposed on the on the physical environment i'm not stuff like that is going to be super powerful so we got to still build out all those kind of applications and the use cases around it but i don't know i think it's gonna be a pretty wild next few years around this i mean i just i'm actually almost at a loss of words this is just so incredible this is truly incredible i hope that people like watching this can get a glimpse of like how incredible it is it really feels like we're in the same room like there is that i guess there's an uncanny valley that seems to have been crossed here like it looks like you yeah like i mean i do there's still a bunch of tuning that i think we'll want to do where different people emote to different extents right so i think one of the big questions is you know like when you smile how wide is your smile and how wide do you want your smile to be and i think getting that to be tuned on a per person basis is is going to be one of the the things that we've that we're going to need to figure out you know it's like to what extent do you want to give people control over that you know some people might try to you know might might prefer a version of themselves that's more emotive in their avatar than their actual faces you know so for example you know i always get a lot of critique and shit for for for having like a relatively stiff expression but you know i mean i might i might feel pretty happy but just make a pretty small smile so i mean maybe you know for me i would i it's actually you know it's like i'd want to have my avatar really be able to better express um like how i'm feeling than than what than how i can do physically so i think that there's a question about how you want to tune that but but overall yeah i mean you we want to start from the baseline of capturing how people actually emote and express themselves and i mean i think the the initial version of this is has been pretty impressive and like you said i do think we're we're kind of beyond the the uncanny valley here where it and it does feel like you it doesn't feel it doesn't feel weird or anything like that i mean that's gonna be the meme that the two most monotone people are in a metaverse together but i think that actually makes it more difficult like the amazing thing here is that the subtleties of the expression of the eyes you know people say i'm monotone and emotionless but i'm not it's just this maybe my expression of emotion is more subtle usually like with the eyes and that's one of the things i've noticed is just how expressive the subtle movement of the corners of the eyes are in terms of displaying happiness or boredom or all that kind of stuff i am curious to see just i've never done one of these before i've never done a podcast as is one of these codec avatars and i'm curious to see what what how what people think of it because you know one of the issues that we've had in some of the vr and mixed reality work is it tends to feel a lot more profound when you're in it than the two d videos capturing the experience so i think that this one because it's photore",
  "duration": 126.75972247123718
}