{
  "transcription": "you've had some strong statements technical statements about the future of artificial intelligence recently throughout your career actually but recently as well you've said that auto aggressive llms are not the way we're going to make progress towards superhuman intelligence these are the large language models like gpt for like llama two and threes and so on how do they work and why are they not going to take us all the way for a number of reasons the first is that there is a number of characteristics of intelligent behavior for example the capacity to understand the world understand the physical world the ability to remember and retrieve things persistent memory the ability to reason and the ability to plan those are four essential characteristics of intelligent systems or entities humans animals lnms can do none of those or they can only do them in a very primitive way and they don't really understand the physical world they don't really have persistent memory they can't really reason and they certainly can't plan and so you know if you expect a system to become intelligent just you know without having the possibility of doing those things you're making a mistake that is not to say that autoregressive llms are not useful they're certainly useful that they're not interesting that we can't build a whole ecosystem of applications around them of course we can but as a path towards human level intelligence they're missing essential components and then there is another tidbit or fact that i think is very interesting those lms are trained on enormous amounts of text basically the entirety of all publicly available text on the internet right that's typically on the order of ten to the thirteen tokens each token is typically two bytes so that's two ten to the thirteen bytes as training data it would take you or me a hundred and seventy thousand years to just read through this at eight hours a day so it seems like an enormous amount of knowledge right that those systems can accumulate but then you realize it's really not that much data if you talk to developmental psychologists and they tell you a four year old has been awake for sixteen thousand hours in his or her life and the amount of information that has reached the visual cortex of that child in four years is about ten to fifteen bytes and you can compute this by estimating that the optical nerve carry about twenty megabytes per second roughly and so ten to the fifteen bytes for a four year old versus two times ten to the thirteen bytes for a hundred and seventy thousand years worth of reading what that tells you is that through sensory input we see a lot more information than we than we do through language and that despite our intuition most of what we learn and most of our knowledge is through our observation and interaction with the real world not through language everything that we learn in the first few years of life and certainly everything that animals learn has nothing to do with language so it would be good to maybe push against some of the intuition behind what you're saying so it is true there's several orders of magnitude more data coming into the human mind much faster and the human mind is able to learn very quickly from that filter the data very quickly somebody might argue your comparison between sensory data versus language that language is already very compressed it already contains a lot more information than the bytes it takes to store them if you compare it to visual data so there's a lot of wisdom in language there's words and the way we stitch them together it already contains a lot of information so is it possible that language alone already has enough wisdom and knowledge in there to be able to from that language construct a world model an understanding of the world an understanding of the physical world that you're saying all lambs lack so it's a big debate among philosophers and also cognitive scientists like whether intelligence needs to be grounded in reality i'm clearly in the camp that yes intelligence cannot appear without some grounding in some reality it doesn't need to be you know physical reality it could be simulated but the environment is just much richer than what you can express in language language is a very approximate representation of our percepts and our mental models right i mean there's a lot of tasks that we accomplish where we manipulate a mental model of the situation at hand and that has nothing to do with language everything that's physical mechanical whatever when we build something when we accomplish a task motor task of you know grabbing something etc we plan or action sequences and we do this by essentially imagining the result of the outcome of a sequence of actions that we might imagine and that requires mental models that don't have much to do with language and that's i would argue most of our knowledge is derived from that interaction with the physical world so a lot of a lot of my colleagues who are more interested in things like computer vision are really on that camp that ai needs to be embodied essentially and then other people coming from the nlp side or maybe have you know some other motivation don't necessarily agree with that and philosophers are split as well and the complexity of the world is hard to it's hard to imagine it you know it's hard to represent all the complexities that we take completely for granted in the real world that we don't even imagine require intelligence right this is the old maraveck paradox from the pioneer of robotics hans maraveck who said you know how is it that with computers it seems to be easy to do high level complex tasks like playing chess and solving integrals and doing things like that whereas the thing we take for granted that we do every day like i don't know learning to drive a car or you know grabbing an object we can't do with computers and you know we have that can pass the bar exam so they must be smart but then they can't learn to drive in twenty hours like any seventeen year old they can't learn to clear out the dinner table and fill up the dishwasher like any ten year old can learn in one shot why is that like you know what what are we missing what type of learning or reasoning architecture or whatever are we missing that basically prevent us from from you know having level five surving cars and domestic robots can a large language model construct a world model that does know how to drive and does know how to fill a dishwasher but just doesn't know how to deal with visual data at this time so it can operate a space of concepts so yeah that's what a lot of people are working on so the answer the short answer is no and the more complex answer is you can use all kinds of tricks to get an llm to basically digest visual representations of representations of images or video or audio for that matter and a classical way of doing this is you train a vision system in some way and we have a number of ways to train vision systems either supervised semi supervised self supervised all kinds of different ways that will turn any image into a high level representation basically a list of tokens that are really similar to the kind of tokens that typical llm takes as an input and then you just feed that to the llm in addition to the text and you just expect it alm to kind of you know during training to kind of be able to use those representations to help make decisions i mean there's been working along those lines for quite a long time and now you see those systems right i mean there are lms that can that have some vision extension but they're basically hacks in the sense that those things are not like trained end to end to handle to really understand the world they're not trained with video for example they don't really understand intuitive physics at least not at the moment so you don't think there's something special to you about intuitive physics about sort of common sense reasoning about the physical space about physical reality that to you is a giant leap that lms are just not able to do we're not going to be able to do this with the type of lms that we are working with today and there's a number of reasons for this but the main reason is the way lms are trained is that you take a piece",
  "duration": 128.54169464111328
}