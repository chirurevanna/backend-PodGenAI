{
  "transcription": " You mentioned something in a stream about the philosophical nature of time. So let's start with a wild question. Do you think time is an illusion? You know, I sell phone calls to Kama for $1,000. And some guy called me and like, you know, it's $1,000. You can talk to me for half an hour. And he's like, yeah, okay. So like time doesn't exist. And I really wanted to share this with you. I'm like, oh, what do you mean time doesn't exist? Right? Like I think time is a useful model, whether it exists or not. Right? Like does quantum physics exist? Well, it doesn't matter. It's about whether it's a useful model to describe reality. Is time maybe compressive? Do you think there is an objective reality or is everything just useful models? Like, underneath it all, is there an actual thing that we're constructing models for? I don't know. I was hoping you would know. I don't think it matters. I mean, this kind of connects to the models of constructive reality with machine learning, right? Sure. Like, is it just nice to have useful approximations of the world such that we can do something with it? So there are things that are real. Colonograph complexity is real. Yeah. Yeah. The compressive thing. Math is real. Yeah. Should be a t-shirt. And I think hard things are actually hard. I don't think P equals NP. Ooh, strong words. Well, I think that's the majority. I do think factoring is in P, but. I don't think you're the person that follows the majority in all walks of life. Well, for that one, I do. Yeah. In theoretical computer science, you're one of the sheep. All right. But to you, time is a useful model. Sure. What were you talking about on the stream with time? Are you made of time? If I remembered half the things I said on stream. Someday someone's going to make a model of all of that, and it's going to come back to haunt me. Someday soon? Yeah, probably. Would that be exciting to you or sad that there's a George Hots model? I mean, the question is when the George Hots model is better than George Hots. Like I am declining and the model is growing. What is the metric by which you measure better or worse in that if you're competing with yourself? Maybe you can just play a game where you have the George Hott's answer and the George Hott's model answer and ask which people prefer. People close to you or strangers? Either one. It will hurt more when it's people close to me, but both will be overtaken by the George Hott's model. It'd be quite painful, right? Loved ones, family members would rather have the model over for Thanksgiving than you. Yeah. Or like significant others would rather sext with the large language model version of you. Especially when it's fine-tuned to their preferences. Yeah. Well, that's what we're doing in a relationship, right? We're just fine-tuning ourselves, but we're inefficient with it because we're selfish and greedy and so on. All language models can fine-tune more efficiently, more selflessly. There's a Star Trek Voyager episode where, you know, Catherine Janeway, lost in the Delta Quadrant, makes herself a lover on the holodeck. And the lover falls asleep on her arm and he snores a little bit. And Janeway edits the program to remove that. And then, of course, the realization is, wait, this person's terrible. It is actually all their nuances and quirks and slight annoyances that make this relationship worthwhile. But I don't think we're going to realize that until it's too late. Well, I think a large language model could incorporate the flaws and the quirks and all that kind of stuff. Just the perfect amount of quirks and flaws to make you charming without crossing the line. Yeah. Yeah. And that's probably a good, like, approximation of the, like, the percent of time the language model should be cranky or an asshole or jealous or all this kind of stuff. And of course it can and it will, but all that difficulty at that point is artificial. There's no more real difficulty. Okay. What's the difference between real and artificial? Artificial difficulty is difficulty that's like constructed or could be turned off with a knob. Real difficulty is like you're in the woods and you've got to survive. So if something can not be turned off with a knob, it's real. Yeah, I think so. Or, I mean, you can't get out of this by smashing the knob with a hammer. I mean, maybe you kind of can. You know, into the wild when, you know, Alexander Supertramp, he wants to explore something that's never been explored before. But it's the 90s. Everything's been explored. So he's like, well, I'm just not going to bring a map. Yeah. I mean, no, you're not exploring. You should have brought a map, dude. You died. There was a bridge a mile from where you were camping. How does that connect to the metaphor of the knob? By not bringing the map, you didn't become an explorer. You just smashed the thing. Yeah. Yeah. The difficulty is still artificial. You failed before you started. What if we just don't have access to the knob? Well, that maybe is even scarier. We already exist in a world of nature, and nature has been fine-tuned over billions of years. To have humans build something and then throw the knob away in some grand romantic gesture is horrifying. Do you think of us humans as individuals that are like born and die, or are we just all part of one living organism that is Earth, that is nature? I don't think there's a clear line there. I think it's all kind of just fuzzy. I don't know. I mean, I don't think I'm conscious. I don't think I'm anything. I think I'm just a computer program. So it's all computation. Everything running in your head is just computation. Everything running in the universe is computation, I think. I believe the extended church-tiring thesis. Yeah, but there seems to be an embodiment to your particular computation. Like, there a consistency. Well, yeah, but I mean models have consistency too. Yeah. Models that have been RLHFed will continually say, you know, like, well, how do I murder ethnic minorities? Oh, well, I can't let you do that, Al. There's a consistency to that behavior. It's all RLHF. Like we RLHF each other. We provide human feedback and thereby fine tune these little pockets of computation. But it's still unclear why that pocket of computation stays with you for years. You have this consistent set of physics, biology, whatever you call the neurons firing. The electrical signals, the mechanical signals, all of that, that seems to stay there. And it contains information, it stores information, and that information permeates through time and stays with you. There's memory. There's sticky Okay, to be fair, like a lot of the models we're building today are very, even RLHF is nowhere near as complex as the human loss function. Reinforcement learning with human feedback. You know, when I talked about will GPT-12 be AGI, my answer is no, of course not. I mean, cross-entropy loss is never going to get you there. You need probably RL in fancy environments in order to get something that would be considered like AGI-like. So to ask like the question about like why, I don't know, like it's just some quirk of evolution, right? I don't think there's anything particularly special about where I ended up, where humans ended up. So, okay. We have human level intelligence. Would you call that AGI? Whatever we have? GI? Look, actually, I don't really even like the word AGI, but general intelligence is defined to be whatever humans have. Okay. So why can GPT-12 not get us to AGI? Can we just like linger on that? If your loss function is categorical cross entropy, if your loss function is just try to maximize compression, I have a SoundCloud, I rap, and I tried to get ChatGPT to help me write raps. And the raps that it wrote sounded like YouTube comment raps. You know, you can go on any rap beat online and you can see what people put in the comments and it's the most like mid quality rap you can find it's made good or bad mid is bad it's like mid it's like every time i talk to you i learn new words mid yeah i was like uh is it is it like basic is that what mid means kind of it's like it's like middle of the curve, right? So there's like that intelligence curve. And you have like the dumb guy, the smart guy, and then the mid guy. Actually, being the mid guy is the worst. The smart guy is like, I put all my money in Bitcoin. The mid guy is like, you can't put money in Bitcoin. It's not real money. And all of it is a genius meme. That's another interesting one. Memes. The humor, the idea, the absurdity encapsulated in a single image. And it just kind of propagates virally between all of our brains. I didn't get much sleep last night, so I'm very, I sound like I'm high. I swear I I'm not do you think we have ideas or ideas have us I think that we're gonna get super scary memes once the AIs actually are superhuman like the gay I will generate memes of course you think it'll make humans laugh",
  "duration": 36.63627290725708
}