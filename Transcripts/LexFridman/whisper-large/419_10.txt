{
  "transcription": " Take me through the OpenAI board saga that started on Thursday, November 16th, maybe Friday, November 17th for you. That was definitely the most painful professional experience of my life. And chaotic and shameful and upsetting and a bunch of other negative things. There were great things about it too, and I wish it had not been in such an adrenaline rush that I wasn't able to stop and appreciate them at the time. I came across this old tweet of mine, or this tweet of mine from that time period, which was like, it was like, you know, kind of going to your own eulogy, watching people say all these great things about you and just like unbelievable support from people I love and care about. That was really nice. That whole weekend, I kind of like felt, with one big exception, I felt a great deal of love and very little hate. Even though it felt like I have no idea what's happening and what's going to happen here, and this feels really bad. There were definitely times I thought it was going to be one of the worst things to ever happen for AI safety. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was going to be something crazy and explosive that happened. But there may be more crazy and explosive things still to happen. It still, I think, helped us build up some resilience and be ready for more challenges in the future. But the thing you had a sense that you would experience is some kind of power struggle. The road to AGI should be a giant power struggle. Like the world should, well, not should, I expect that to be the case. And so you have to go through that, like you said, iterate as often as possible, figuring out how to have a board structure, how to have organization, how to have the kind of people that you're working with, how to communicate all that in order to de-escalate the power struggle as much as possible. Yeah. Pacify it. But at this point, it feels, you know, like something that was in the past that was really unpleasant and really difficult and painful. But we're back to work and things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after. There was like this fugue state for kind of like the month after, maybe 45 days after. That was, I was just sort of like drifting through the days. I was so out of it. I was feeling so down. Just at a personal psychological level. Yeah. Really painful. And hard to like have to keep running open AI in the middle of that. I just wanted to like crawl into a cave and kind of recover for a while. But, you know, now it's like we're just back to working on the mission. Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research and product development and money and all this kind of stuff, so that you, who have a very high potential of building AGI, would do so in a slightly more organized, less dramatic way in the future. So there's value there to go, both the personal psychological aspects of you as a leader and also just the board structure and all this kind of messy stuff. Definitely learned a lot about structure and incentives and what we need out of a board. And I think that is, it is valuable that this happened now in some sense. I think this is probably not like the last high stress moment of OpenAI, but it was quite a high stress moment. My company very nearly got destroyed. And we think a lot about many of the other things we've got to get right for AGI, but thinking about how to build a resilient org and how to build a structure that will stand up to a sudden it escalates and why don't we fire Sam kind of thing? I think, I think the board members were, are well-meaning people on the whole. And I believe that in stressful situations where people feel time pressure or whatever, people understandably make suboptimal decisions. And I think one of the challenges for OpenAI will be we're going to have to have a board and a team that are good at operating under pressure. Do you think the board had too much power? I think boards are supposed to have a lot of power. But one of the things that we did see is in most corporate structures, boards are usually answerable to shareholders. Sometimes people have super voting shares or whatever. In this case, and I think one of the things with our structure that we maybe should have thought about more than we did, is that the board of a nonprofit has, unless you put other rules in place, like quite a lot of power, they don't really answer to anyone but themselves. And there's ways in which that's good, but what we'd really like is for the board of OpenAI to answer to the world as a whole as much as that's a practical thing. So there's a new board announced? Yeah. There's, I guess, a new smaller board at first, and now there's a new final board. Not a final board yet. We've added some, we'll add more. Added some, okay. What is fixed in the new one that was perhaps broken in the previous one? The old board sort of got smaller over the course of about a year. It was nine and then it went down to six. And then we couldn't agree on who to add. And the board also, I think, didn't have a lot of experienced board members. And a lot of the new board members at Open AI have just have more experience as board members. I think that'll help. It's been criticized, some of the people that are added to the board. I heard a lot of people criticizing the addition of Larry Summers, for example. What's the process of selecting the board like? What's involved in that? So Brett and Larry were kind of decided in the heat of the moment over this like very tense weekend. And that was – I mean that weekend was like a real roller coaster. It was like a lot of ups and downs. And we were trying to agree on new board members that both sort of the executive team here and the old board members felt would be reasonable. Larry was actually one of their suggestions, the old board members. Brett, I think I had even previous to that weekend suggested, but he was busy and didn't want to do it. And then we really needed help and would. We talked about a lot of other people too, but that was, I felt like if I was going to come back, I needed new board members. I didn't think I could work with the old board again in the same configuration, although we then decided, and I'm grateful that Adam would stay, but we wanted to get to, we considered various configurations, decided we wanted to get to a board of three and had to find two new board members over the course of sort of a short period of time. So those were decided honestly without – that's like you kind of do that on the battlefield. You don't have time to design a rigorous process then. For new board members since – new board members we'll add going forward, we have some criteria that we think are important for the board to have, different expertise that we want the board to have. Unlike hiring an executive where you need them to do one role well, the board needs to do a whole role of kind of governance and thoughtfulness well. And so one thing that Brett says, which I really like, is that, you know, we want to hire board members in slates, not as individuals one at a time. And, you know, thinking about a group of people that will bring nonprofit expertise, expertise at running companies, sort of good legal and governance expertise, that's kind of what we've tried to optimize for. So is technical savvy important for the individual board members? Not for every board member, but for certainly some you need that. That's part of what the board needs to do. So, I mean, the interesting thing that people probably don't understand about OpenAI, I certainly don't, is like all the details of running the business.",
  "duration": 18.14714026451111
}