{
  "transcription": "What to you is the probability that super intelligent AI will destroy all human civilization? What's the time frame? Let's say 100 years. In the next hundred years. So the problem of controlling AGI or superintelligence, in my opinion, is like a problem of creating a perpetual safety machine by analogy with perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT 5, 6, 7, but they just keep improving, learning, eventually self modifying, interacting with the environment, interacting with malevolent actors. The difference between cybersecurity, narrow AI safety and safety for general AI for superintelligence is that we don't get a second chance with cybersecurity. Somebody hacks your account, what's the big deal? You get a new password, new credit card, you move on."
}