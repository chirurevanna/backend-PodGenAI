{
  "transcription": "You've had some strong statements, technical statements about the future of artificial intelligence recently, throughout your career actually, but recently as well you've said that auto aggressive LLMs are not the way we're going to make progress towards superhuman intelligence. These are the large language models like GPT4, like llama 2 and 3 soon and so on. How do they work and why are they not going to take us all away? For a number of reasons. The first is that there is a number of characteristics of intelligent behavior. For example, the capacity to understand the world, understand the physical world, the ability to remember and retrieve things, persistent memory, the ability to reason and the ability to plan. Those are four essential characteristics of intelligent systems or entities. Humans, animals. LLMs can do none of those, or they can only do them in a very primitive way. And they don't really understand the physical world, they don't really have persistent memory, they can't really reason and they certainly can't plan. And so if you expect a system to become intelligent just without having the possibility of doing those things, you're making a mistake. That is not to say that autoregressive LLMs are not useful. They're certainly useful. That they're not interesting, that we can't build a whole ecosystem of applications around them. Of course we can. But as a path towards human level intelligence, they're missing essential components. And then there is another tidbit or fact that I think is very interesting. Those LLMs are trained on enormous amounts of text. Basically the entirety of all publicly available text on the Internet, right? That's typically on the order of 10 to the 13 tokens. Each token is typically 2 bytes. So that's 2, 10 to the 13 bytes as training data. It would take you or me 170,000 years to just read through this at eight hours a day. So it seems like an enormous amount of knowledge that those systems can accumulate. But then you realize it's really not that much data. If you talk to developmental psychologists and they tell you a 4 year old has been awake for 16,000 hours in his or her life and the amount of information that has reached the visual cortex of that child in four years is about 10 to the 15 bytes. And you can compute this by estimating that the optical nerve carry about 20 megabytes per second, roughly. And so 10 to the 15 bytes for a 4 year old versus 2 times 10 to the 13 bytes for 170,000 years worth of reading. That tells you is that through sensory input we see a lot more information than we do through language, and that despite our intuition, most of what we learn and most of our knowledge is through our observation and interaction with the real world, not through language. Everything that we learn in the first few years of life, and certainly everything that animals learn, has nothing to do with language. So it'd be good to maybe push against some of the intuition behind what you're saying. So it is true there's several orders of magnitude more data coming into the human mind much faster. And the human mind is able to learn very quickly from that. Filter the data very quickly. You know, somebody might argue your comparison between sensory data versus language. That language is already very compressed. It already contains a lot more information than the bytes it takes to store them if you compare it to visual data. So there's a lot of wisdom and language, there's words, and the way we stitch them together, it already contains a lot of information. So is it possible that language alone already has enough wisdom and knowledge in there to be able to, from that language, construct a world model, an understanding of the world, an understanding of the physical world that you're saying all lens lack? So it's a big debate among philosophers and also cognitive scientists, like, whether intelligence needs to be grounded in reality. I'm clearly in the camp that, yes, intelligence cannot appear without some grounding in some reality. It doesn't need to be.",
  "duration": 13.785619735717773
}